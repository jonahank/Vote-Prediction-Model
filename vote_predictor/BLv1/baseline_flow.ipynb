{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./party_lst.json') as json_file:\n",
    "    parties = json.load(json_file)\n",
    "parties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow for each party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party = \"Socialistisk Folkeparti\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./by_party_train/\" + party + \".csv\")\n",
    "df_test = pd.read_csv(\"./by_party_test/\" + party + \".csv\")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_vote(df):\n",
    "  \n",
    "  df['label'] = df.apply(lambda x: 1 if x['Neutral']<x['Against']>x['For'] else 0 if x['Neutral']<x['For']>x['Against'] else None, axis=1)\n",
    "\n",
    "  df.rename(columns={'description':'text'}, inplace=True)\n",
    "  df = df[['text', 'label']]\n",
    "  df = df.dropna()\n",
    "  df['label'] = df.apply(lambda x: int(x['label']), axis=1)\n",
    "\n",
    "  return df\n",
    "\n",
    "## Note \"Against\" = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = preprocess_vote(df_train)\n",
    "df_test = preprocess_vote(df_test)\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset.from_pandas(df_train, preserve_index=True)\n",
    "test_set = Dataset.from_pandas(df_test, preserve_index=True)\n",
    "dataset = DatasetDict({'train':train_set,\n",
    "                        'test':test_set})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "klimaBERT = \"/klimaBERTe4_v2.1\"\n",
    "checkpoint = klimaBERT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokenized_batch = tokenizer(batch['text'], padding=True, truncation=True, max_length=512)\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Steps for processing data\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "## Rename label column to labels, if not already done\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets.column_names\n",
    "## Our model needs columns that it already knows (the 4 below, NOT any custom columns like \"text\"):\n",
    "#['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    roc_auc = roc_auc_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc_score': roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\", per_device_train_batch_size=2, num_train_epochs=5, seed=2022, save_strategy=\"no\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) #Change num  of labels if needed\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set to use GPU\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models_binary_party/\"+party)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a roc curve for a predictive model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot\n",
    "from sklearn.dummy import DummyClassifier\n",
    "m = torch.nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weight_pred = trainer.predict(tokenized_datasets[\"test\"])[0]\n",
    "weight_for = []\n",
    "weight_against = []\n",
    "for input in weight_pred:\n",
    "    input2 = torch.from_numpy(input)\n",
    "    (i,k) = m(input2) #i=against, k=for\n",
    "    weight_for.append(i.item())\n",
    "    weight_against.append(k.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC CURVE\n",
    "testy = tokenized_datasets[\"test\"][\"labels\"]\n",
    "\n",
    "# plot no skill roc curve\n",
    "model = DummyClassifier(strategy='stratified')\n",
    "model.fit(tokenized_datasets[\"train\"], tokenized_datasets[\"train\"][\"labels\"])\n",
    "yhat = model.predict_proba(tokenized_datasets[\"test\"])\n",
    "pos_probs = yhat[:, 1]\n",
    "roc_auc_noskill = roc_auc_score(testy, pos_probs)\n",
    "\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
    "\n",
    "# plot the skill curve\n",
    "# retrieve just the probabilities for the positive class\n",
    "pos_probs = weight_against\n",
    "# calculate roc curve for model\n",
    "fpr, tpr, _ = roc_curve(testy, pos_probs)\n",
    "\n",
    "# plot model roc curve\n",
    "pyplot.plot(fpr, tpr, marker='.', label=f'Baseline-{party}')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "roc_auc = roc_auc_score(testy, pos_probs)\n",
    "print('No Skill ROC AUC %.3f' % roc_auc_noskill)\n",
    "print('Baseline ROC AUC %.3f' % roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PR Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "y = tokenized_datasets[\"test\"]\n",
    "no_of_pos = y[\"labels\"].sum().item()\n",
    "\n",
    "\n",
    "# calculate the no skill line as the proportion of the positive class\n",
    "no_skill = (no_of_pos / len(y))\n",
    "# plot the no skill precision-recall curve\n",
    "model = DummyClassifier(strategy='stratified')\n",
    "model.fit(tokenized_datasets[\"train\"], tokenized_datasets[\"train\"][\"labels\"])\n",
    "yhat = model.predict_proba(tokenized_datasets[\"test\"])\n",
    "pos_probs = yhat[:, 1]\n",
    "# calculate the precision-recall auc\n",
    "precision, recall, _ = precision_recall_curve(testy, pos_probs)\n",
    "auc_score_noskill = auc(recall, precision)\n",
    "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "\n",
    "# calculate model precision-recall curve\n",
    "pos_probs = weight_against\n",
    "precision, recall, _ = precision_recall_curve(testy, pos_probs)\n",
    "# plot the model precision-recall curve\n",
    "pyplot.plot(recall, precision, marker='.', label=f'Baseline-{party}')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "auc_score = auc(recall, precision)\n",
    "print('No Skill PR AUC: %.3f' % auc_score_noskill)\n",
    "print('Logistic PR AUC: %.3f' % auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for all parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for party in parties:\n",
    "    df_train = pd.read_csv(\"./by_party_train\" + party + \".csv\")\n",
    "    df_test = pd.read_csv(\"./by_party_test\" + party + \".csv\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c394272594a68b0a5bd03bbb8469a449c9402c12c87bbd02bc40bbbc1d6f707"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
