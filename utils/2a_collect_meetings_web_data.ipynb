{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean raw html data\n",
    "Notebook ```scrape-data.ipynb``` is used for scraping raw html data.\n",
    "\n",
    "Arguments are:\n",
    "- YYYYY, XX = 20121, 105\n",
    "- YYYYY, XX = 20131, 109\n",
    "- YYYYY, XX = 20141, 99\n",
    "- YYYYY, XX = 20151, 112\n",
    "- YYYYY, XX = 20161, 112\n",
    "- YYYYY, XX = 20171, 108\n",
    "- YYYYY, XX = 20181, 99\n",
    "- YYYYY, XX = 20191, 152\n",
    "- YYYYY, XX = 20201, 43 (until Dec 29 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize speeches__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notes__\n",
    "* 20141, file 99 is removed because \"mødet er aflyst\"\n",
    "* 20181, files 94,95,96,97,98,99 are removed because \"mødet er aflyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('danish')\n",
    "#y_, iter_ = 20101,108\n",
    "#y_, iter_ = 20111,102\n",
    "#y_, iter_ = 20121,105 # <---- something is strangely formatted here\n",
    "#y_, iter_ = 20131,109\n",
    "#y_, iter_ = 20141,99\n",
    "#y_, iter_ = 20151,112\n",
    "#y_, iter_ = 20161,112\n",
    "#y_, iter_ = 20171,108\n",
    "#y_, iter_ = 20181,99\n",
    "#y_, iter_ = 20191,152\n",
    "#y_, iter_ = 20201,138 #not complete, disregard all data from 27 and up (not completed) under uderbejdelse - need to redownload\n",
    "y_, iter_ = 20211,75\n",
    "\n",
    "odd = set()\n",
    "# go through files\n",
    "#for ii in range(1,iter_+1):\n",
    "for ii in [2]:\n",
    "    \n",
    "    # list for outputting\n",
    "    dta = []\n",
    "\n",
    "    # load url data\n",
    "    fileIn = open('../data_collection/meetings/url_data/%d-%03d.html' % (y_,ii),'r')\n",
    "    htmlText = fileIn.read()\n",
    "    fileIn.close()\n",
    "    \n",
    "    # soupify it \n",
    "    soup = BeautifulSoup(htmlText, 'html.parser')\n",
    "    \n",
    "    # date of speech\n",
    "    t = datetime.strptime(soup.find_all('span', attrs={'class':'video_date'})[0].text, '%d-%m-%Y kl. %H:%M')\n",
    "    \n",
    "    # go through individual speakers\n",
    "    for speech in soup.find_all('div', attrs={'class': 'video-item-referat'}):\n",
    "        \n",
    "        # only if there is text\n",
    "        if speech.text.strip() != '':\n",
    "        \n",
    "            # keep track of speakers\n",
    "            speaker = speech.text.split('\\n')[0].strip()\n",
    "            speaker = speaker[speaker.index(u'\\xa0')+1:]\n",
    "            print(speaker)\n",
    "\n",
    "            # find start_index, slice string and disregard name of speaker\n",
    "            start_index = speech.text.index('\\n')\n",
    "\n",
    "            # clean string\n",
    "            txt = re.sub(r'\\d+|[\\.!?,-]', '', speech.text.lower()[start_index:])\n",
    "            \n",
    "            # some more cleaning\n",
    "            txt = re.sub(r'\\d+|[/]', ' ', txt)\n",
    "\n",
    "            # tokenize and remove stopwords\n",
    "            #txt = [w for w in nltk.word_tokenize(txt) if len(w) > 2 and w not in stopwords]\n",
    "            \n",
    "            # keep text in original format\n",
    "            txt = nltk.word_tokenize(txt)\n",
    "            \n",
    "            # save to list\n",
    "            dta.append((speaker,t.strftime('%Y-%m-%d H%H:%M'),','.join(txt)))\n",
    "        \n",
    "    # transform list to dataframe\n",
    "    df = pd.DataFrame(dta,columns=['speaker','date','tokens'])\n",
    "    #df = df.replace({u'\\xf8':u'\\xc3\\xb8'},regex=True)\n",
    "    # handle encoding\n",
    "    types = df.apply(lambda x: pd.api.types.infer_dtype(x.values))\n",
    "    for col in types[types=='unicode'].index:\n",
    "        df[col] = df[col].apply(lambda x: x.encode('utf-8').strip())\n",
    "    # output tsv-file\n",
    "    df.to_csv('tokenized_data/%d-%03d.tsv' % (y_,ii),sep='\\t',index=False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data by creating dataframes of all uncleaned speeches per year\n",
    "This is a much better way of processing the data, than what is used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_ in ['20101', '20111']:\n",
    "    \n",
    "    # find all files from that specific year\n",
    "    files_ = sorted([f for f in os.listdir('../data_collection/meetings/url_data/') if f[:5] == y_])\n",
    "\n",
    "    # which classes to extract text from\n",
    "    p_class = ['Tekst','TekstLuft','TekstIndryk','Pind','PindTekst']\n",
    "\n",
    "    # append speeces to empty list\n",
    "    data = []\n",
    "\n",
    "    # go through files\n",
    "    for file_ in files_:\n",
    "        try:\n",
    "            # load url data\n",
    "            fileIn = open('../data_collection/meetings/url_data/' + file_,'r')\n",
    "            htmlText = fileIn.read()\n",
    "            fileIn.close()\n",
    "\n",
    "            # soupify it \n",
    "            soup = BeautifulSoup(htmlText, 'html.parser')\n",
    "\n",
    "            # date of speech\n",
    "            t = datetime.strptime(soup.find_all('span', attrs={'class':'video_date'})[0].text, '%d-%m-%Y kl. %H:%M')\n",
    "\n",
    "            # go through individual speeches\n",
    "            for speech in soup.find_all('div', attrs={'class': 'video-item-referat'}):\n",
    "\n",
    "                # only if there is text\n",
    "                if speech.text.strip() != '':\n",
    "\n",
    "                    # keep track of speakers\n",
    "                    #speaker = speech.text.split('\\n')[0].strip()\n",
    "                    #speaker = speaker[speaker.index(u'\\xa0')+1:]\n",
    "                    speaker =  speech.findAll('a', attrs = {'href':['#pv']})[0].text.strip()\n",
    "\n",
    "                    # extract text\n",
    "                    text = ' '.join([i.text.strip() for i in speech.findAll('p', attrs={'class' : p_class})])\n",
    "                    text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "                    # save to list\n",
    "                    data.append((speaker,t.strftime('%Y-%m-%d H%H:%M'),text))\n",
    "\n",
    "\n",
    "                    # to find strange classes of paragraphs\n",
    "                    #test = Counter() # move further up when running\n",
    "                    #da_fuq = [] # move further up when running\n",
    "                    #         for i in speech.findAll(\"p\"):\n",
    "                    #             for j in i['class']:\n",
    "                    #                 test[j] += 1\n",
    "                    #                 if j not in {'Tekst','TekstIndryk','TekstLuft'}:\n",
    "                    #                     da_fuq.append(speech)\n",
    "\n",
    "            # transform to dataframe\n",
    "            df = pd.DataFrame(data,columns=['speaker','time','text'])\n",
    "\n",
    "            # save to csv file\n",
    "            df.to_csv('../data_collection/meetings/unprocessed_text/' + y_ + '.csv',index=False)\n",
    "\n",
    "        except:\n",
    "            print(file_)\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
