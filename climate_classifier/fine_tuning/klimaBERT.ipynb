{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets\n",
    "path = \"./ft_meetings_votes_995_covid.csv\"\n",
    "df = pd.read_csv(path) #data_collection/votes_data_cleaned.pkl\n",
    "dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "dataset = dataset.remove_columns('Unnamed: 0')\n",
    "\n",
    "# 80% train, 18% test + 2% validation\n",
    "train_test = dataset.train_test_split(test_size=0.2, seed=2022)\n",
    "\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test['test']}\n",
    "    )\n",
    "\n",
    "dataset = train_test_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 796\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"label\"]==0])\n",
    "len(df[df[\"label\"]==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "Aelaectra = \"Maltehb/aelaectra-danish-electra-small-cased\"\n",
    "BERT = \"Maltehb/danish-bert-botxo\"\n",
    "\n",
    "checkpoint = BERT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokenized_batch = tokenizer(batch['text'], padding=True, truncation=True, max_length=512)\n",
    "    return tokenized_batch\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Steps for processing data\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "## Rename label column to labels, if not already done\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets.column_names\n",
    "## Our model needs columns that it already knows (the 4 below, NOT any custom columns like \"text\"):\n",
    "#['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model specification and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    # Open a file with access mode 'a'\n",
    "    file_object = open('training_metrics.txt', 'a')\n",
    "    # Append at the end of \n",
    "    result = json.dumps({\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    })\n",
    "    file_object.write(result + '\\n')\n",
    "    # Close the file\n",
    "    file_object.close()\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                    \"test-trainer\",\n",
    "                    evaluation_strategy=\"epoch\",\n",
    "                    per_device_train_batch_size=2,\n",
    "                    num_train_epochs=4,\n",
    "                    save_strategy='no'\n",
    "                    seed=2019,\n",
    "                    )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set to use GPU\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save\n",
    "trainer.save_model(\"/klimaBERTe11_v2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load (part1)\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_metric\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "checkpoint2 = \"/klimaBERT_v2\"\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(checkpoint2)\n",
    "training_args2 = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(checkpoint2, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load (part2)\n",
    "trainer2 = Trainer(\n",
    "    model2,\n",
    "    tokenizer=tokenizer2,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict using the loaded model\n",
    "predictions2 = trainer2.predict(tokenized_datasets[\"test\"])\n",
    "print(predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Twitter Data\n",
    "\n",
    "This Twitter dataset is from huggingface's data library. It does not in particular contain climate-related material, but more broad material from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"labeled_tw138_testing.csv\")\n",
    "\n",
    "#uncomment if you want to test on only climate quotes\n",
    "#df = df[df['label']==1]\n",
    "\n",
    "test_set = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels = test_set['label']\n",
    "test_set.rename_column(\"label\", \"original_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Twitter data and tokenize it\n",
    "dataset_twitter = test_set\n",
    "tokenized_datasets_twitter = dataset_twitter.map(tokenize, batched=True)\n",
    "tokenized_datasets_twitter.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictions:\n",
    "#%%capture\n",
    "tokenized_datasets_twitter\n",
    "predictions = trainer.predict(tokenized_datasets_twitter)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "\n",
    "for (i,j) in predictions[0]:\n",
    "  if i > j: label.append(\"non-climate\")\n",
    "  else: label.append(\"climate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare y_real with y_pred\n",
    "df_compare = pd.DataFrame()\n",
    "df_compare = dataset_twitter.to_pandas()\n",
    "df_compare[\"y_pred\"] = label\n",
    "df_compare['original_label'] = list_of_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_twitter_climate = df_compare[df_compare[\"y_pred\"]==\"non-climate\"].reset_index()\n",
    "print(\"y_pred:climate\",len(df_twitter_climate.index))\n",
    "print(\"y_pred:non-climate\",len(df_compare.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing Falsely labelled non-climate quotes\n",
    "for i in range(len(df_twitter_climate['text'])):\n",
    "  print(df_twitter_climate['text'][i], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load (part1)\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_metric\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "checkpoint = \"/klimaBERTe4_v2.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "## Load trainer API\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.33ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    tokenized_batch = tokenizer(batch['text'], padding=True, truncation=True, max_length=512)\n",
    "    return tokenized_batch\n",
    "\n",
    "dataset = dataset[\"test\"]\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Downloading builder script: 5.76kB [00:00, 764kB/s]                    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 4.7510056, -3.6382744],\n",
       "       [ 4.6295094, -3.6536262],\n",
       "       [ 4.636514 , -3.5109625],\n",
       "       [-3.8254824,  3.5311162],\n",
       "       [ 4.570917 , -3.5767603],\n",
       "       [-4.205848 ,  3.7701135],\n",
       "       [ 4.658006 , -3.5874648],\n",
       "       [ 4.8381867, -3.8025331],\n",
       "       [-4.7697635,  4.045868 ],\n",
       "       [ 4.7213674, -3.8322732],\n",
       "       [-4.5998554,  4.095766 ],\n",
       "       [ 4.6287847, -3.6518412],\n",
       "       [-4.696137 ,  3.9940534],\n",
       "       [-3.2632928,  3.12307  ],\n",
       "       [ 4.6697454, -3.7154021],\n",
       "       [-4.4340196,  3.8638675],\n",
       "       [-4.534858 ,  3.919992 ],\n",
       "       [ 4.6017013, -3.6463678],\n",
       "       [ 4.6675735, -3.6929398],\n",
       "       [ 4.728398 , -3.5579088],\n",
       "       [ 4.426664 , -3.553609 ],\n",
       "       [ 4.685045 , -3.6837354],\n",
       "       [-2.6666918,  2.5495653],\n",
       "       [ 4.5215707, -3.5741937],\n",
       "       [ 4.6695957, -3.6921668],\n",
       "       [ 4.5658965, -3.5805566],\n",
       "       [ 4.626716 , -3.6279554],\n",
       "       [ 4.616471 , -3.5807369],\n",
       "       [ 4.5933685, -3.6979885],\n",
       "       [ 4.4807687, -3.5582514],\n",
       "       [ 3.8438835, -3.2189279],\n",
       "       [-4.338409 ,  3.9310555],\n",
       "       [-4.523839 ,  3.9397068],\n",
       "       [-4.572143 ,  3.9748127],\n",
       "       [ 4.278389 , -3.413485 ],\n",
       "       [-4.690724 ,  3.9322135],\n",
       "       [ 4.661177 , -3.5964384],\n",
       "       [-4.307421 ,  3.7816296],\n",
       "       [-4.3475313,  3.726931 ],\n",
       "       [-4.2787585,  3.7951393],\n",
       "       [ 4.685852 , -3.565947 ],\n",
       "       [ 4.4272246, -3.5399735],\n",
       "       [ 4.718558 , -3.6213303],\n",
       "       [ 4.573083 , -3.6149569],\n",
       "       [-4.7005415,  3.8648458],\n",
       "       [-4.5814443,  3.9752562],\n",
       "       [-4.5933313,  4.005571 ],\n",
       "       [ 4.650911 , -3.6352625],\n",
       "       [-4.5128617,  3.8818526],\n",
       "       [ 4.5345664, -3.525604 ],\n",
       "       [-3.5660553,  3.3951936],\n",
       "       [-4.6694174,  3.9821067],\n",
       "       [ 4.490826 , -3.6668525],\n",
       "       [-4.52632  ,  3.948404 ],\n",
       "       [ 4.695704 , -3.5624847],\n",
       "       [ 4.54827  , -3.6084948],\n",
       "       [ 4.6522117, -3.5649502],\n",
       "       [-3.9520292,  3.667933 ],\n",
       "       [-4.6471844,  3.9558895],\n",
       "       [ 0.9683273,  0.3674808],\n",
       "       [-3.8475995,  3.5195165],\n",
       "       [ 4.600929 , -3.6401465],\n",
       "       [ 4.52029  , -3.5517173],\n",
       "       [ 4.6415234, -3.6263337],\n",
       "       [-4.590257 ,  3.9042356],\n",
       "       [-4.535273 ,  3.9408386],\n",
       "       [-4.501834 ,  3.914872 ],\n",
       "       [-4.116402 ,  3.7689347],\n",
       "       [ 4.5672097, -3.4826164],\n",
       "       [ 4.592202 , -3.6769333],\n",
       "       [ 3.9451842, -3.0900426],\n",
       "       [ 4.6295094, -3.6536262],\n",
       "       [ 4.623568 , -3.626157 ],\n",
       "       [ 4.7121916, -3.6030116],\n",
       "       [-4.3740764,  3.8444312],\n",
       "       [-4.6881113,  3.9385047],\n",
       "       [-4.67384  ,  3.9549992],\n",
       "       [-3.1867294,  3.0534472],\n",
       "       [ 4.751203 , -3.7168744],\n",
       "       [ 4.692823 , -3.659657 ],\n",
       "       [ 4.6655755, -3.5699306],\n",
       "       [ 4.6083884, -3.5240262],\n",
       "       [ 4.6396823, -3.6064124],\n",
       "       [-3.2581413,  2.9644048],\n",
       "       [ 4.717453 , -3.5959005],\n",
       "       [ 3.227227 , -2.1708708],\n",
       "       [ 4.668036 , -3.6389818],\n",
       "       [ 4.676592 , -3.6228638],\n",
       "       [-4.585603 ,  4.004432 ],\n",
       "       [ 4.648107 , -3.5837638],\n",
       "       [ 4.5837884, -3.5905478],\n",
       "       [-4.723385 ,  3.990671 ],\n",
       "       [ 4.3655896, -3.4578886],\n",
       "       [ 4.7259336, -3.750183 ],\n",
       "       [ 4.751781 , -3.6948402],\n",
       "       [ 4.6275816, -3.539432 ],\n",
       "       [ 4.675421 , -3.6997445],\n",
       "       [-4.515804 ,  3.803271 ],\n",
       "       [ 4.4810987, -3.60036  ],\n",
       "       [ 4.38436  , -3.5830083],\n",
       "       [ 4.6613965, -3.6485229],\n",
       "       [ 4.590382 , -3.6374512],\n",
       "       [ 4.672873 , -3.6760416],\n",
       "       [-4.626713 ,  4.0102124],\n",
       "       [ 4.444672 , -3.5656605],\n",
       "       [ 4.6068845, -3.6537218],\n",
       "       [ 4.631057 , -3.5967274],\n",
       "       [ 4.711387 , -3.738041 ],\n",
       "       [-4.1425095,  3.7453523],\n",
       "       [-4.740414 ,  4.1328907],\n",
       "       [ 4.6862245, -3.568267 ],\n",
       "       [-4.6418614,  4.0171494],\n",
       "       [ 4.6239753, -3.6104202],\n",
       "       [ 4.627208 , -3.6504872],\n",
       "       [ 4.578996 , -3.5772188],\n",
       "       [ 4.629008 , -3.5132685],\n",
       "       [-3.7355237,  3.5923662],\n",
       "       [ 4.2034087, -3.5542488],\n",
       "       [ 4.5581264, -3.4601557],\n",
       "       [ 4.666958 , -3.5912828],\n",
       "       [-3.9351413,  3.576632 ],\n",
       "       [ 4.579162 , -3.4004831],\n",
       "       [-4.683146 ,  3.983752 ],\n",
       "       [-4.527989 ,  3.8749807],\n",
       "       [ 4.711255 , -3.64495  ],\n",
       "       [-4.5470147,  3.9886806],\n",
       "       [ 4.6842237, -3.518833 ],\n",
       "       [-4.6831594,  3.936915 ],\n",
       "       [ 4.7194166, -3.652096 ],\n",
       "       [-4.578934 ,  3.9007916],\n",
       "       [ 4.4957356, -3.6019125],\n",
       "       [-4.656099 ,  3.9628973],\n",
       "       [ 4.6799455, -3.665177 ],\n",
       "       [ 4.656527 , -3.6534634],\n",
       "       [ 4.587326 , -3.5070577],\n",
       "       [-3.8103502,  3.5158987],\n",
       "       [-4.5093646,  3.8971815],\n",
       "       [-4.539942 ,  3.9865777],\n",
       "       [-4.624256 ,  3.8983817],\n",
       "       [-4.7129655,  3.8878753],\n",
       "       [ 4.4135804, -3.650776 ],\n",
       "       [ 4.6055636, -3.576542 ],\n",
       "       [ 4.654624 , -3.5449486],\n",
       "       [-4.666265 ,  4.0413804],\n",
       "       [-2.763734 ,  1.9833604],\n",
       "       [-4.613139 ,  3.9634836],\n",
       "       [-4.4769125,  3.8918962],\n",
       "       [-4.6757636,  3.884796 ],\n",
       "       [-4.072236 ,  3.6100984],\n",
       "       [-4.0307636,  3.6651437],\n",
       "       [ 4.5915074, -3.5526674],\n",
       "       [-4.675165 ,  4.043515 ],\n",
       "       [ 4.5817375, -3.3017445],\n",
       "       [ 4.7118764, -3.662182 ],\n",
       "       [-3.8623366,  3.5239549],\n",
       "       [ 4.493846 , -3.549366 ],\n",
       "       [ 4.603897 , -3.3572898],\n",
       "       [-4.6052737,  3.9962692],\n",
       "       [ 4.6515155, -3.543573 ],\n",
       "       [-4.6858864,  3.9863813],\n",
       "       [-4.2532835,  3.591724 ],\n",
       "       [-4.7059875,  3.9302974],\n",
       "       [-4.705937 ,  4.1493106],\n",
       "       [-4.7242455,  4.0183344],\n",
       "       [ 4.7170734, -3.8193572],\n",
       "       [ 4.8174186, -3.7934337],\n",
       "       [-4.6388125,  3.9298284],\n",
       "       [ 4.432547 , -3.6547723],\n",
       "       [ 4.765406 , -3.69739  ],\n",
       "       [ 4.580543 , -3.5832915],\n",
       "       [ 4.5524282, -3.5629094],\n",
       "       [-4.778482 ,  4.0601916],\n",
       "       [ 4.555033 , -3.5913918],\n",
       "       [ 4.572089 , -3.6387267],\n",
       "       [-0.5197145,  0.662827 ],\n",
       "       [-4.4703803,  3.9710164],\n",
       "       [-4.5248675,  3.8927512],\n",
       "       [ 4.566992 , -3.6013074],\n",
       "       [-4.669693 ,  4.0798163],\n",
       "       [ 4.6216598, -3.6350107],\n",
       "       [ 4.4854007, -3.6140883],\n",
       "       [ 4.404934 , -3.5111294],\n",
       "       [-4.4007607,  3.9302242],\n",
       "       [ 4.5386415, -3.5245454],\n",
       "       [-1.1093289,  0.9685869],\n",
       "       [ 4.187468 , -3.2698743],\n",
       "       [-4.2951   ,  3.7930756],\n",
       "       [ 4.7514815, -3.812581 ],\n",
       "       [-4.720026 ,  4.0057006],\n",
       "       [ 4.7112107, -3.6291223],\n",
       "       [-4.668944 ,  4.016413 ],\n",
       "       [-4.7222605,  4.0128236],\n",
       "       [-4.523839 ,  3.9397068],\n",
       "       [-4.377749 ,  3.809882 ],\n",
       "       [ 4.607469 , -3.5388434],\n",
       "       [ 4.6722035, -3.668959 ],\n",
       "       [-4.781654 ,  4.032997 ],\n",
       "       [ 4.3442607, -3.5158896],\n",
       "       [ 4.7539053, -3.775799 ],\n",
       "       [ 4.612144 , -3.5258188]], dtype=float32), label_ids=array([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0], dtype=int64), metrics={'test_loss': 0.004497433081269264, 'test_accuracy': 1.0, 'test_f1': 1.0, 'test_runtime': 379.2225, 'test_samples_per_second': 0.527, 'test_steps_per_second': 0.066})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on sub-set\n",
    "predictions2 = trainer.predict(tokenized_datasets)\n",
    "predictions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a roc curve for a predictive model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot\n",
    "import torch\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "m = torch.nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'For': 0, 'Imod': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = []\n",
    "\n",
    "for (i,j) in predictions2[0]:\n",
    "  if i > j: label.append(\"non-climate\")\n",
    "  else: label.append(\"climate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the predictions of the model on the holdout test set\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "lab2int = {\"non-climate\":0, \"climate\":1}\n",
    "y_test2 =  dataset[\"label\"]\n",
    "y_pred2 = [lab2int[item] for item in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 1.0 / AUC: 1.0 / F1: 1.0 / F1-macro: 1.0 / F1-micro 1.0  / F1-weight 1.0 \n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(y_test2, y_pred2)\n",
    "precision = precision_score(y_test2, y_pred2)\n",
    "recall = recall_score(y_test2, y_pred2)\n",
    "f1 = f1_score(y_test2, y_pred2)\n",
    "f1_macro = f1_score(y_test2, y_pred2,average=\"macro\")\n",
    "f1_micro = f1_score(y_test2, y_pred2,average=\"micro\")\n",
    "f1_weight = f1_score(y_test2, y_pred2,average=\"weighted\")\n",
    "print('Precision: {} / Recall: {} / AUC: {} / F1: {} / F1-macro: {} / F1-micro {}  / F1-weight {} '.format(\n",
    "    round(precision, 2), round(recall, 2), round(auc,2), round(f1,2), round(f1_macro,2), round(f1_micro,2), round(f1_weight,2)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc9de61b87691b5c2df4faa10d03c8011b5237e1331d96144d761a181cf79b3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
