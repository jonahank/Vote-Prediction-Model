{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets\n",
    "path = \"./ft_meetings_votes_995_covid.csv\"\n",
    "df = pd.read_csv(path) #data_collection/votes_data_cleaned.pkl\n",
    "dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "dataset = dataset.remove_columns('Unnamed: 0')\n",
    "\n",
    "# 80% train, 18% test + 2% validation\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_test['test'].train_test_split(test_size=0.10)\n",
    "\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "dataset = train_test_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "Aelaectra = \"Maltehb/aelaectra-danish-electra-small-cased\"\n",
    "BERT = \"Maltehb/danish-bert-botxo\"\n",
    "\n",
    "checkpoint = BERT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokenized_batch = tokenizer(batch['text'], padding=True, truncation=True, max_length=512)\n",
    "    return tokenized_batch\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Steps for processing data\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "## Rename label column to labels, if not already done\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets.column_names\n",
    "## Our model needs columns that it already knows (the 4 below, NOT any custom columns like \"text\"):\n",
    "#['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model specification and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    # Open a file with access mode 'a'\n",
    "    file_object = open('training_metrics.txt', 'a')\n",
    "    # Append at the end of \n",
    "    result = json.dumps({\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    })\n",
    "    file_object.write(result + '\\n')\n",
    "    # Close the file\n",
    "    file_object.close()\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                    \"test-trainer\",\n",
    "                    evaluation_strategy=\"epoch\",\n",
    "                    per_device_train_batch_size=2,\n",
    "                    num_train_epochs=4,\n",
    "                    save_strategy='no'\n",
    "                    seed=2019,\n",
    "                    )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set to use GPU\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save\n",
    "trainer.save_model(\"/klimaBERTe11_v2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load (part1)\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_metric\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "checkpoint2 = \"/klimaBERT_v2\"\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(checkpoint2)\n",
    "training_args2 = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(checkpoint2, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load (part2)\n",
    "trainer2 = Trainer(\n",
    "    model2,\n",
    "    tokenizer=tokenizer2,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict using the loaded model\n",
    "predictions2 = trainer2.predict(tokenized_datasets[\"test\"])\n",
    "print(predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Twitter Data\n",
    "\n",
    "This Twitter dataset is from huggingface's data library. It does not in particular contain climate-related material, but more broad material from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"labeled_tw138_testing.csv\")\n",
    "\n",
    "#uncomment if you want to test on only climate quotes\n",
    "#df = df[df['label']==1]\n",
    "\n",
    "test_set = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels = test_set['label']\n",
    "test_set.rename_column(\"label\", \"original_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Twitter data and tokenize it\n",
    "dataset_twitter = test_set\n",
    "tokenized_datasets_twitter = dataset_twitter.map(tokenize, batched=True)\n",
    "tokenized_datasets_twitter.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictions:\n",
    "#%%capture\n",
    "tokenized_datasets_twitter\n",
    "predictions = trainer.predict(tokenized_datasets_twitter)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "\n",
    "for (i,j) in predictions[0]:\n",
    "  if i > j: label.append(\"non-climate\")\n",
    "  else: label.append(\"climate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare y_real with y_pred\n",
    "df_compare = pd.DataFrame()\n",
    "df_compare = dataset_twitter.to_pandas()\n",
    "df_compare[\"y_pred\"] = label\n",
    "df_compare['original_label'] = list_of_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_twitter_climate = df_compare[df_compare[\"y_pred\"]==\"non-climate\"].reset_index()\n",
    "print(\"y_pred:climate\",len(df_twitter_climate.index))\n",
    "print(\"y_pred:non-climate\",len(df_compare.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing Falsely labelled non-climate quotes\n",
    "for i in range(len(df_twitter_climate['text'])):\n",
    "  print(df_twitter_climate['text'][i], \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c394272594a68b0a5bd03bbb8469a449c9402c12c87bbd02bc40bbbc1d6f707"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
